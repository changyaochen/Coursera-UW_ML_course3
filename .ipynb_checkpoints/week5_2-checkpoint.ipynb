{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, graphviz, sys\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122607, 5)\n"
     ]
    }
   ],
   "source": [
    "loans = pd.read_csv('lending-club-data.csv', low_memory=False)\n",
    "\n",
    "loans['bad_loans'].value_counts()\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x: 1-x)\n",
    "del loans['bad_loans']\n",
    "\n",
    "target = 'safe_loans'\n",
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "loans = loans[features + [target]]\n",
    "print(loans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122607, 5)\n"
     ]
    }
   ],
   "source": [
    "loans = loans.dropna()\n",
    "print(loans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode\n",
    "for name, dtype in zip(loans.dtypes.index, loans.dtypes.values):\n",
    "    if dtype == 'object':\n",
    "        onehot = pd.get_dummies(loans[name])\n",
    "        onehot.rename(columns={x:name+'_'+x.strip().replace(' ', '_') for x in onehot.columns}\n",
    "                  , inplace=True)\n",
    "        loans = loans.join(onehot)\n",
    "        del loans[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_onehot = [x for x in loans.columns if x != target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('module-8-assignment-2-train-idx.json', 'r') as f:\n",
    "    train_idx = json.load(f)\n",
    "with open('module-8-assignment-2-test-idx.json', 'r') as f:\n",
    "    test_idx = json.load(f)\n",
    "    \n",
    "train_data = loans.iloc[train_idx, :].reset_index(drop=True)\n",
    "test_data = loans.iloc[test_idx, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_36_months</th>\n",
       "      <th>term_60_months</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_2_years</th>\n",
       "      <th>emp_length_3_years</th>\n",
       "      <th>emp_length_4_years</th>\n",
       "      <th>emp_length_5_years</th>\n",
       "      <th>emp_length_6_years</th>\n",
       "      <th>emp_length_7_years</th>\n",
       "      <th>emp_length_8_years</th>\n",
       "      <th>emp_length_9_years</th>\n",
       "      <th>emp_length_&lt;_1_year</th>\n",
       "      <th>emp_length_n/a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \\\n",
       "0           0        0        0        1        0        0        0        0   \n",
       "1           0        0        0        0        0        0        1        0   \n",
       "2           0        0        1        0        0        0        0        0   \n",
       "3           0        0        0        1        0        0        0        0   \n",
       "4           0        0        1        0        0        0        0        0   \n",
       "\n",
       "   term_36_months  term_60_months       ...        emp_length_2_years  \\\n",
       "0               0               1       ...                         0   \n",
       "1               0               1       ...                         0   \n",
       "2               0               1       ...                         0   \n",
       "3               1               0       ...                         0   \n",
       "4               1               0       ...                         0   \n",
       "\n",
       "   emp_length_3_years  emp_length_4_years  emp_length_5_years  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   1                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   1                   0                   0   \n",
       "\n",
       "   emp_length_6_years  emp_length_7_years  emp_length_8_years  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   emp_length_9_years  emp_length_<_1_year  emp_length_n/a  \n",
       "0                   0                    1               0  \n",
       "1                   0                    0               0  \n",
       "2                   0                    1               0  \n",
       "3                   0                    1               0  \n",
       "4                   0                    0               0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37224, 26)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "safe_loans  grade_A\n",
       "0           0          17218\n",
       "            1           1258\n",
       "1           0          14876\n",
       "            1           3872\n",
       "Name: grade_A, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby(target)[\"grade_A\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum([i for (i, v) in \n",
    "                                 zip(data_weights, [x == +1 for x in labels_in_node]) if v])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    total_weight_negative = sum([i for (i, v) in \n",
    "                                 zip(data_weights, [x == 0 for x in labels_in_node]) if v])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    if total_weight_negative > total_weight_positive:\n",
    "        return (weighted_mistakes_all_negative, 0)\n",
    "    else:\n",
    "        return (weighted_mistakes_all_positive, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33333333333333331, 1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_in_node = [1,1,1,0,0,1]\n",
    "data_weights = [0.5, 0.5, 0.5, 1, 0.9, 0.5]\n",
    "data_weights = np.ones(len(labels_in_node)) / len(labels_in_node)\n",
    "intermediate_node_weighted_mistakes(labels_in_node, data_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data is identical in each feature, this function should return None\n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0].reset_index(drop=True)\n",
    "        right_split = data[data[feature] == 1].reset_index(drop=True)\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        left_data_weights = np.array([data_weights[i] for \n",
    "                                      i,x in enumerate(data[feature] == 0) if x])\n",
    "        right_data_weights = np.array([data_weights[i] for \n",
    "                                      i,x in enumerate(data[feature] == 1) if x])\n",
    "#         print(\"len of total:\", len(data_weights))\n",
    "#         print(\"len of left-w: \", len(left_data_weights))\n",
    "#         print(\"len of right-w: \", len(right_data_weights))\n",
    "                    \n",
    "        # DIFFERENT HERE\n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(\n",
    "                            left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(\n",
    "                            right_split[target], right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) \n",
    "        # / [total weight of all data points]\n",
    "        error = (left_weighted_mistakes + right_weighted_mistakes) / sum(data_weights)\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "#     print(\"best error is: \", best_error)\n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    leaf['prediction'] = best_class\n",
    "    \n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, \n",
    "                                  current_depth=1, max_depth=10, verbose=True):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    if verbose: \n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        if verbose: print(\"Stopping condition 1 reached.\")                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        if verbose: print(\"Stopping condition 2 reached.\")                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        if verbose: print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = np.array([w for (w, c) in \n",
    "            zip(data_weights, [x for x in data[splitting_feature] == 0]) if c])\n",
    "    right_data_weights = np.array([w for (w, c) in \n",
    "            zip(data_weights, [x for x in data[splitting_feature] == 1]) if c])\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Split on feature %s. (%s, %s)\" % (\n",
    "              splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        if verbose: print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        if verbose: print(\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, \n",
    "        left_data_weights, current_depth + 1, max_depth, verbose)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, \n",
    "        right_data_weights, current_depth + 1, max_depth, verbose)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate=False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print(\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x), axis=1)\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_36_months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Split on feature grade_A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9122 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (101 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Split on feature grade_D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (23300 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4701 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "example_data_weights = np.ones(len(train_data))\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, \n",
    "                                            feature_onehot, target,\n",
    "                                            example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12\n",
       "1     8\n",
       "Name: safe_loans, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 30\n",
    "sanity_data = train_data.sample(N, random_state=42)\n",
    "example_data_weights = np.random.random(N)\n",
    "example_data_weights /= sum(example_data_weights)\n",
    "sanity_data[target][:20].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333331"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tree = weighted_decision_tree_create(sanity_data, \n",
    "                                    feature_onehot, target,\n",
    "                                    example_data_weights, max_depth=3, verbose=False)\n",
    "evaluate_classification_error(test_tree, sanity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40003761014399314"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data.iloc[-10:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data\n",
    "    alpha = np.ones(len(data)) / len(data)\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in range(num_tree_stumps):\n",
    "        print('\\n=====================================================')\n",
    "        print('Adaboost Iteration %d' % t)\n",
    "        print('=====================================================')        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, \n",
    "                                                   data_weights=alpha, max_depth=1,\n",
    "                                                   verbose=False)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis=1)\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        weighted_error = sum([alpha[i] for i,x in enumerate(is_wrong) if x])/float(sum(alpha))\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        weight = 0.5*log((1 - weighted_error)/float(weighted_error))\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = np.array([exp(-weight) if x else exp(weight) for x in is_correct])\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment\n",
    "        # Then normalize data points weights\n",
    "        alpha_old = alpha[:]\n",
    "        alpha = np.array([x*y for (x, y) in zip(alpha, adjustment)])\n",
    "        alpha /= float(sum(alpha))\n",
    "        \n",
    "        print(\"\\n***** stump summary *****\")\n",
    "        print(\"is_correct count:   \", is_correct.sum())\n",
    "        print(\"is_wrong count:     \", is_wrong.sum())\n",
    "        print(\"w-error is:         \", weighted_error)\n",
    "        print(\"tree weight is:     \", weight)\n",
    "        print(\"min, max of adjustment: \", adjustment.min(), adjustment.max())\n",
    "        print(\"min, max alpha:     \", alpha.min(), alpha.max())\n",
    "        print(\"is alpha the same?  \", np.allclose(alpha, alpha_old))\n",
    "        p_c_w_tuple = [(p, c, '{:.2f}'.format(w)) for (p, c, w) in zip(predictions, data[target], alpha)]\n",
    "#         print(\"detail: {}\".format(p_c_w_tuple))\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_36_months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    21529\n",
      "is_wrong count:      15695\n",
      "w-error is:          0.421636578552\n",
      "tree weight is:      0.15802933659181698\n",
      "min, max of adjustment:  0.853824733293 1.17120055324\n",
      "min, max alpha:      2.32244879001e-05 3.18572793883e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    21090\n",
      "is_wrong count:      16134\n",
      "w-error is:          0.412498248915\n",
      "tree weight is:      0.17682363293569195\n",
      "min, max of adjustment:  0.837927554315 1.19342059448\n",
      "min, max alpha:      1.97654627047e-05 3.861504803e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    20141\n",
      "is_wrong count:      17083\n",
      "w-error is:          0.453574664308\n",
      "tree weight is:      0.09311888971170464\n",
      "min, max of adjustment:  0.911085175203 1.09759221993\n",
      "min, max alpha:      1.80861514041e-05 4.25674658095e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19828\n",
      "is_wrong count:      17396\n",
      "w-error is:          0.463619975982\n",
      "tree weight is:      0.07288885525814356\n",
      "min, max of adjustment:  0.929704156012 1.07561098177\n",
      "min, max alpha:      1.68594565366e-05 4.59077132293e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    20156\n",
      "is_wrong count:      17068\n",
      "w-error is:          0.466518644019\n",
      "tree weight is:      0.06706306914168735\n",
      "min, max of adjustment:  0.935136221238 1.06936291985\n",
      "min, max alpha:      1.58013549561e-05 4.92024421937e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19955\n",
      "is_wrong count:      17269\n",
      "w-error is:          0.467760207218\n",
      "tree weight is:      0.06456916961637442\n",
      "min, max of adjustment:  0.937471267491 1.06669935888\n",
      "min, max alpha:      1.68904437704e-05 4.62220627441e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    21090\n",
      "is_wrong count:      16134\n",
      "w-error is:          0.472746758716\n",
      "tree weight is:      0.05456055779251551\n",
      "min, max of adjustment:  0.946901164877 1.05607642813\n",
      "min, max alpha:      1.60173920688e-05 4.88867050825e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19536\n",
      "is_wrong count:      17688\n",
      "w-error is:          0.478258250407\n",
      "tree weight is:      0.04351093673426624\n",
      "min, max of adjustment:  0.957422082963 1.04447141736\n",
      "min, max alpha:      1.54266772665e-05 5.08548206699e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    21090\n",
      "is_wrong count:      16134\n",
      "w-error is:          0.485509702974\n",
      "tree weight is:      0.028988711500811302\n",
      "min, max of adjustment:  0.971427430362 1.02941297388\n",
      "min, max alpha:      1.4992194562e-05 5.23726100204e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19075\n",
      "is_wrong count:      18149\n",
      "w-error is:          0.487021661048\n",
      "tree weight is:      0.025962509691607592\n",
      "min, max of adjustment:  0.974371618416 1.02630247136\n",
      "min, max alpha:      1.46128924202e-05 5.37682553048e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    17083\n",
      "is_wrong count:      20141\n",
      "w-error is:          0.484617644977\n",
      "tree weight is:      0.030774421492271706\n",
      "min, max of adjustment:  0.969694290597 1.03125284917\n",
      "min, max alpha:      1.50767234454e-05 5.21634615357e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    20156\n",
      "is_wrong count:      17068\n",
      "w-error is:          0.48778378868\n",
      "tree weight is:      0.024437285973060677\n",
      "min, max of adjustment:  0.97585888704 1.02473832363\n",
      "min, max alpha:      1.54543096709e-05 5.09193777772e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19075\n",
      "is_wrong count:      18149\n",
      "w-error is:          0.486229749089\n",
      "tree weight is:      0.027547467959314356\n",
      "min, max of adjustment:  0.972828503274 1.02793040771\n",
      "min, max alpha:      1.50400978292e-05 5.23614380575e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_4_years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    18671\n",
      "is_wrong count:      18553\n",
      "w-error is:          0.488914307103\n",
      "tree weight is:      0.02217501979807067\n",
      "min, max of adjustment:  0.978069038625 1.02242271303\n",
      "min, max alpha:      1.47138709205e-05 5.35486866479e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19075\n",
      "is_wrong count:      18149\n",
      "w-error is:          0.491328105076\n",
      "tree weight is:      0.017345529206822804\n",
      "min, max of adjustment:  0.98280403846 1.01749683647\n",
      "min, max alpha:      1.44630272159e-05 5.44938159396e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    18060\n",
      "is_wrong count:      19164\n",
      "w-error is:          0.490334137226\n",
      "tree weight is:      0.019334134276889344\n",
      "min, max of adjustment:  0.980851571352 1.01952224904\n",
      "min, max alpha:      1.46019985157e-05 5.39953619771e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    21090\n",
      "is_wrong count:      16134\n",
      "w-error is:          0.492674444173\n",
      "tree weight is:      0.014652160100626073\n",
      "min, max of adjustment:  0.985454660444 1.01476002919\n",
      "min, max alpha:      1.43911521389e-05 5.47982167694e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19536\n",
      "is_wrong count:      17688\n",
      "w-error is:          0.490189134704\n",
      "tree weight is:      0.019624249376590602\n",
      "min, max of adjustment:  0.980567052774 1.01981807075\n",
      "min, max alpha:      1.42554607761e-05 5.53411235289e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_36_months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    15695\n",
      "is_wrong count:      21529\n",
      "w-error is:          0.489549406889\n",
      "tree weight is:      0.02090423064752089\n",
      "min, max of adjustment:  0.979312748226 1.02112425455\n",
      "min, max alpha:      1.45597773948e-05 5.42081097326e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    20156\n",
      "is_wrong count:      17068\n",
      "w-error is:          0.492808481162\n",
      "tree weight is:      0.014384029614075425\n",
      "min, max of adjustment:  0.985718926308 1.01448797757\n",
      "min, max alpha:      1.47722471825e-05 5.34394875695e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19075\n",
      "is_wrong count:      18149\n",
      "w-error is:          0.491256963368\n",
      "tree weight is:      0.017487855788117592\n",
      "min, max of adjustment:  0.982664169274 1.01764166362\n",
      "min, max alpha:      1.45183777653e-05 5.43905649735e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    17083\n",
      "is_wrong count:      20141\n",
      "w-error is:          0.491340019779\n",
      "tree weight is:      0.017321692640064525\n",
      "min, max of adjustment:  0.982827465413 1.01747258313\n",
      "min, max alpha:      1.42786565025e-05 5.53203113996e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19536\n",
      "is_wrong count:      17688\n",
      "w-error is:          0.494329547875\n",
      "tree weight is:      0.01134139049421376\n",
      "min, max of adjustment:  0.988722680628 1.01140594789\n",
      "min, max alpha:      1.41590891231e-05 5.57946435758e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    21090\n",
      "is_wrong count:      16134\n",
      "w-error is:          0.491042934372\n",
      "tree weight is:      0.01791604793600448\n",
      "min, max of adjustment:  0.982243490265 1.01807750309\n",
      "min, max alpha:      1.39099052546e-05 5.68123881542e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19075\n",
      "is_wrong count:      18149\n",
      "w-error is:          0.493457365522\n",
      "tree weight is:      0.013086015871339087\n",
      "min, max of adjustment:  0.98699923377 1.01317201249\n",
      "min, max alpha:      1.37302413537e-05 5.75656501693e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_2_years. (33652, 3572)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33652 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3572 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    18628\n",
      "is_wrong count:      18596\n",
      "w-error is:          0.492718310217\n",
      "tree weight is:      0.014564409290132037\n",
      "min, max of adjustment:  0.985541138683 1.01467098708\n",
      "min, max alpha:      1.35331529111e-05 5.84163902331e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19536\n",
      "is_wrong count:      17688\n",
      "w-error is:          0.494148977125\n",
      "tree weight is:      0.011702579944557772\n",
      "min, max of adjustment:  0.988365628912 1.01177132303\n",
      "min, max alpha:      1.33766191024e-05 5.91080756384e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_OWN. (34149, 3075)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34149 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3075 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    18431\n",
      "is_wrong count:      18793\n",
      "w-error is:          0.493184682982\n",
      "tree weight is:      0.013631478294483391\n",
      "min, max of adjustment:  0.98646100958 1.0137248105\n",
      "min, max alpha:      1.31967391801e-05 5.99248898821e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    19075\n",
      "is_wrong count:      18149\n",
      "w-error is:          0.494252681313\n",
      "tree weight is:      0.01149514366369003\n",
      "min, max of adjustment:  0.988570673068 1.01156146671\n",
      "min, max alpha:      1.30467712754e-05 6.06217145073e-05\n",
      "is alpha the same?   False\n",
      "\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "\n",
      "***** stump summary *****\n",
      "is_correct count:    18060\n",
      "is_wrong count:      19164\n",
      "w-error is:          0.494348454041\n",
      "tree weight is:      0.011303573315145135\n",
      "min, max of adjustment:  0.988760072037 1.01136770009\n",
      "min, max alpha:      1.29009506445e-05 6.13147608855e-05\n",
      "is alpha the same?   False\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(\n",
    "    train_data, feature_onehot, target, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = np.zeros(len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis=1).values\n",
    "        predictions = [1 if x == 1 else -1 for x in predictions]\n",
    "#         print(predictions)\n",
    "        \n",
    "        # Accumulate predictions on scores array\n",
    "        scores = [stump_weights[i]*x + y for (x, y) in zip(predictions, scores)]\n",
    "#         print(scores)\n",
    "        \n",
    "    return [1 if x > 0 else 0 for x in scores], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroes = predict_adaboost(stump_weights, tree_stumps, train_data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011734907782874704"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(scroes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.421636578551\n",
      "Iteration 2, training error = 0.433430045132\n",
      "Iteration 3, training error = 0.400037610144\n",
      "Iteration 4, training error = 0.400037610144\n",
      "Iteration 5, training error = 0.384724908661\n",
      "Iteration 6, training error = 0.384617451107\n",
      "Iteration 7, training error = 0.382763808296\n",
      "Iteration 8, training error = 0.384617451107\n",
      "Iteration 9, training error = 0.382763808296\n",
      "Iteration 10, training error = 0.384483129164\n",
      "Iteration 11, training error = 0.382736943907\n",
      "Iteration 12, training error = 0.381447453256\n",
      "Iteration 13, training error = 0.381528046422\n",
      "Iteration 14, training error = 0.380560928433\n",
      "Iteration 15, training error = 0.380507199656\n",
      "Iteration 16, training error = 0.378223726628\n",
      "Iteration 17, training error = 0.378277455405\n",
      "Iteration 18, training error = 0.378411777348\n",
      "Iteration 19, training error = 0.378062540297\n",
      "Iteration 20, training error = 0.378761014399\n",
      "Iteration 21, training error = 0.379566946056\n",
      "Iteration 22, training error = 0.378895336342\n",
      "Iteration 23, training error = 0.378895336342\n",
      "Iteration 24, training error = 0.378761014399\n",
      "Iteration 25, training error = 0.378895336342\n",
      "Iteration 26, training error = 0.378975929508\n",
      "Iteration 27, training error = 0.379110251451\n",
      "Iteration 28, training error = 0.378922200731\n",
      "Iteration 29, training error = 0.379029658285\n",
      "Iteration 30, training error = 0.378734150011\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "# stump_weights = weights\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)[0]\n",
    "    error = 1.0 - sum(train_data[target] == predictions) / len(train_data)\n",
    "    error_all.append(error)\n",
    "    print(\"Iteration %s, training error = %s\" % (n, error_all[n-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
